Kybelix-Agri-AI: Project Notes (Simple + Complete)

Overview
- Entry point: train.py
- Configuration: config.py
- Dataset: src/data/dataset.py
- Models: src/models/unet.py + src/models/segformer_fpn.py (b0, b4, ghostnet, segformer_b4)
- Loss + metrics: src/utils/losses.py, src/utils/metrics.py

Data Flow
1) Config sets paths, hyperparameters, and runtime options.
2) train.py builds datasets + dataloaders.
3) Model is created by src/models/model_factory.get_model.
4) Training runs with mixed precision on CUDA.
5) Checkpoints and results are written to the configured output folders.

Dataset Structure
- Root folder: ./phase1data (local) or /content/drive/MyDrive/phase1data (Colab)
- Expected layout:
  ./phase1data/unmasked/train
  ./phase1data/unmasked/val
  ./phase1data/unmasked/test
  ./phase1data/masked/train
  ./phase1data/masked/val
- Masks are RGB images; target color is Config.TARGET_COLOR (default [0, 255, 0]).
- Filenames must match by base name (extension ignored).
- The loader does NOT recurse into subfolders.

How To Train
1) Install:
   pip install -r requirements.txt
2) Edit config.py if needed.
3) Run:
   python3 train.py

Augmentations
- Controlled from config.py:
  USE_AUGMENTATION = True/False
  AUGMENTATION_PRESET = "none" | "light" | "strong"

Where Runs Are Saved
- If Google Drive is mounted and SAVE_RUNS_TO_DRIVE=True:
  /content/drive/MyDrive/kybelix_runs/checkpoints
  /content/drive/MyDrive/kybelix_runs/results
- Otherwise:
  ./checkpoints
  ./results

Checkpoint Policy (current)
- Drive: only *_last.pth and *_best.pth
- Local: per-epoch checkpoints if SAVE_EPOCHS_LOCALLY=True

Resume Training
- By default, each run folder is named like:
  b4_YYYYMMDD_HHMMSS
- To resume, set in config.py:
  RESUME_PATH = "/content/drive/MyDrive/kybelix_runs/checkpoints/<run_name>/b4_last.pth"
  RESET_OPTIMIZER = False
- Or CLI:
  python3 train.py --resume /content/drive/MyDrive/kybelix_runs/checkpoints/<run_name>/b4_last.pth

Early Stopping + Scheduler
- Controlled from config.py:
  EARLY_STOPPING, EARLY_STOPPING_PATIENCE, EARLY_STOPPING_MIN_DELTA
  EARLY_STOPPING_MONITOR (val_iou or val_loss)
  SCHEDULER (plateau or None), SCHEDULER_PATIENCE, SCHEDULER_FACTOR, SCHEDULER_MIN_LR

Models
- b0: EfficientNet-B0 encoder + attention UNet decoder (lighter, faster).
- b4: EfficientNet-B4 encoder + attention UNet decoder (heavier, higher accuracy).
- ghostnet: lightweight backbone for speed and memory efficiency.
- segformer_b4: MiT-B4 (SegFormer encoder) + FPN-style multi-scale fusion + auxiliary boundary head.

Loss (current default)
- Dice(seg) + CrossEntropy(seg) + lambda * BCE(edge)
- edge target: Sobel (or morph gradient, configurable in config.py)
- boundary head is used as training regularizer; inference uses segmentation output.
